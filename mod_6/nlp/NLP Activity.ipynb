{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this morning's task, we will be working with policy text scraped from the Warren and Sanders campaigns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from the data folder. \n",
    "# We will be working mainly with the 'Policy' feature\n",
    "\n",
    "df = pd.read_csv('data/2020_policies_feb_24.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question set 1:\n",
    "After remove punctuation and ridding the text of numbers and other low semantic value text, answer the following questions.\n",
    "\n",
    "1. Which document has the greatest average word length?\n",
    "2. What is the average word length of the entire corpus?\n",
    "3. Which is greater, the average word length for the documents in the Warren or Sanders campaigns? \n",
    "\n",
    "For each question set, Slack out your answers, which will vary depending on which preprocessing choices you make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proceed through the remaining standard preprocessing steps in whatever manner you see fit. Make sure to:\n",
    "- Make text lowercase\n",
    "- Remove stopwords\n",
    "- Stem or lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question set 2:\n",
    "1. What are the most common words across the corpus?\n",
    "2. What are the most common words across each campaign?\n",
    "\n",
    "> in order to answer these questions, you may find the nltk FreqDist function helpful.\n",
    "\n",
    "3. Use the FreqDist plot method to make a frequency plot for the corpus as a whole.  \n",
    "4. Based on that plot, should any more words be added to our stopword library?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question set 3:\n",
    "\n",
    "1. What are the most common bigrams in the corpus?\n",
    "2. What are the most common bigrams in the Warren campain and the Sanders campaign, respectively?\n",
    "3. Answer questions 1 and 2 for trigrams.\n",
    "\n",
    "> Hint: You may find it useful to leverage the nltk.collocations functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
